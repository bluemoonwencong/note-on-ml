# 本笔记不在共同进度之内

*作者@棒棒*

## Week 1: 2018.04.30 ---- 2018.05.06

## 内容：监督学习：线性回归、逻辑回归、决策树、朴素贝叶斯、集成学习



### *线性回归*

- 定义：给定一个点集，能够用一条曲线去拟合，如果这个曲线是直线，则称为线性回归
- 方法：最小二乘法、梯度下降法、岭回归、LASSO

#### 1. 最小二乘法

- 基于均方误差最小化获得参数估计结果

- 适用条件

  - 数据量比较少
  - 矩阵满秩满秩

  在满足上面两个条件时，通过矩阵运算可以较快得到解析解。

#### 2. 梯度下降

​	针对最小二乘法无法得到解析解的情况下，可以利用迭代法进行求解：梯度下降（一次逼近）、牛顿（二次逼近）

- 计算代价函数当前位置的梯度计算下一次参数更新的方向

- 梯度下降分类

  - 批量梯度下降(BGS)

    因为每一次的更新需要利用所有样本的数据进行计算，当数据量**很大**时，迭代速率**很慢**，当然它的迭代过程比较稳定

  - 随机梯度下降(SGD)

    每一次的更新仅利用某一个参数更新，计算速率**快**，但迭代过程比较**曲折**

  - 小批量梯度下降(MSGD) -- 两者的结合

    选取部分样本数据进行参数更新

  - Adagrad 

    前三种方法存在无法实现学习率动态变化的缺点

    Adagrad 利用历史的梯度信息作为学习率的分母实现学习率的动态变化，但是在**训练后期**，由于分母很大会使得学习率趋于0，学习速率**很慢**

#### 3. 岭回归(Ridge Regression)

​	其实是一种改良的最小二乘，抛弃了最小二乘的无偏性，以牺牲部分信息为代价获得回归系数，通过加一个L2正则项来约束代价函数，也就是对要估计的参数进行权衡(即权衡方差) 。岭回归的罚约束项可以收缩待估系数趋于零，但并非是零。这个缺点对模型精度影响不大，但是给模型解释造成了困难（即，岭回归虽然**减少了模型的复杂度**，但并**没有解决**变量的**选择问题**）

#### 4. LASSO

​	一种压缩估计方法，通过构造罚函数得到一个较为精炼的模型，使得它能够压缩一些系数，同时设定一些系数为零，因此保留了子集收缩的优点（**特征选择**），是一种处理复共线性数据的有偏估计。

​	由于LASSO所加的罚约束项为L1范数，导函数不连续（无法进行求导并使用GD），导致在求解回归系数是**计算更为复杂**，在实际应用中，可以利用坐标下降法进行计算。

------

------

### *逻辑回归(Logistic Regression)*

​	与多重线性回归分析有很多相同之处，模型形式基本相同：$wx+b$，区别在于应变量：$y=wx+b$, 而Logistic Regression 是通过将$wx+b$对应于一个隐状态，$p=L(wx+b)$，根据$p$与$1-p$的大小决定应变量的值。如果$L()$是Logistic 函数，则称为逻辑回归。

***逻辑回归虽然叫回归，但是它其实更是一种分类方法***

- 优点：
  - 直接对分类可能性进行建模，无需事先假设数据分布，避免了假设不存在所带来的问题（**判别式模型**）
  - 不是仅预测出**“类别”**，而是得到近似的概率预测，对许多需要用概率辅助的任务很有用
  - 任意阶可导函数，有很好的数学性质




------

------

### *决策树(Decision Tree)*

​	遵循简单直观的分而治之策略，目的是生成一棵泛化能力强的决策树。简单来讲，就是通过if/else的嵌套实现。

#### 1. 核心问题

- 关键是确定**根节点**和**分支准则**
- 停止生长时机的选择

#### 2. 划分准则

##### (1) 信息熵：

度量样本集合纯度常用的一种指标

#####　(2)信息增益（ID3）

##### (3)信息增益率（C4.5）

##### (4) 基尼系数(CART) - Classification and regression tree

#### 3. 减枝策略

##### (1) 预剪枝

##### (2) 后剪枝

#### 4. 连续值与缺失值的处理

#### 5. 多变量决策树

------

------

### *贝叶斯分类器（Bayesian Classification)*

#### 1. 贝叶斯决策论

​	寻找一个判定准则$h$，使得总体风险$R(h)$最小

#### 2. 极大似然估计(MLE)

​	假定具有某种特定的概率分布形式，再基于训练样本对概率分布的参数进行估计

- 模型的训练过程就是参数的估计过程
  - 虽然参数化的估计方法使得类条件概率估计变得简单，但估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的数据分布

#### 3. 朴素贝叶斯分类器

- **最重要的前提：*样本的属性间是相互独立的***


------

------

### *支持向量机 SVM* - Support Vector Machine

#### 1. 间隔与支持向量

找到一个超平面将不同类别的样本分开

超平面线性方程为：

$w^T x + b = 0$

空间中任意样本$x$到超平面的距离为：

$r = \frac{\left | w^T x + b \right |}{\left || w \right ||}$

#### 2. 软件间隔与正则化

- 现实任务中很难确定核函数
- 即使恰好找到某个核函数，很难确定貌似线性可分割的结果不是过拟合造成的

#### 3. 核函数

#### 4.支持向量回归 

------

------

###  *集成学习*  - Ensemble learning

#### 1. 集成学习概述

- 打群架 对已有的学习方法的改进
- 实现方式：e.g. 以分类器为例 (1) 获得一列的分类器；(2) 对这些分类器进行排布

#### 2. bagging 

- 有放回采样：对现有的训练集记性抽样构建训练子集，这些子集可存在交集。然后分别利用构建的子集训练子分类器，再通过Average或者voting的原则对训练出来的子分类器进行整合，从而得到最后的分类器
- 这种方法适用于模型复杂且易过拟合的情况，如**决策树**。

##### 随机森林 （决策树+boosting）

​	鉴于决策树容易过拟合的缺点，随机森林采用多个决策树的投票机制来改善决策树，我们假设随机森林使用了m棵决策树，那么就需要产生m个一定数量的样本集来训练每一棵树，如果用全样本去训练m棵决策树显然是不可取的，全样本训练忽视了局部样本的规律，对于模型的泛化能力是有害的。

**生成方法**

1. 样本中通过重采样产生n个样本
2. 假设样本特征数目为a，对n个样本随机选择a中的k个特征，用建立决策树的方式获得最佳分割点
3. 重复步骤1和2 m次， 产生m棵决策树
4. 多数投票机制进行预测

**总结：它对于多维特征的数据集分类有很高的效率，还可以做特征重要性的选择。运行效率和准确率较高，实现起来也比较简单。但是在数据噪音比较大的情况下会过拟合，过拟合的缺点对于随机森林来说还是较为致命的。**

#### 3. boosting

(对在训练集上表现弱的学习器进行提升；不管学习器有多弱，通过boosting后可以变得很强)

是一种用来提高弱分类算法准确度的方法，这种方法通过构造一个预测函数系列，以一定的方式将他们组合成一个预测函数。它的工作机制：先从初始训练集训练出一个基学习器，再根据机学习器的表现对训练样本分布进行调整，使得先前基学习器**做错的**样本在后续受到更多的关注(**增加权值**)，然后基于调整后的样本分布训练下一个基学习器；如此重复进行，直至基学习器的数目达到指定值T，最终对这T个学习器进行加权结合。

##### AdaBoost

通过训练数据的分布构造一个分类器，然后通过误差率求出这个若弱分类器的权重，通过更新训练数据的分布，迭代进行，直到达到迭代次数或者损失函数小于某一阈值。

- Adaboost中涉及到一些可以进行调整的参数和计算公式的选择主要有以下几点：
  - 弱分类器如何选择 
  - 如何更好的实验误差率计算分类器的系数 
  - 如何更好的计算训练数据的权重的分布 
  - 弱分类器如何进行组合 
  - 迭代次数 
  - 损失函数的阈值选取多少



#### 4. 投票法、平均法 

- 投票法
- 平均法




































### 













