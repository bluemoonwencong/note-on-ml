# 本笔记不在共同进度之内

*作者@棒棒*

## Week 1: 2018.04.30 ---- 2018.05.06

## 内容：监督学习：线性回归、逻辑回归、决策树、朴素贝叶斯



### *线性回归*

- 定义：给定一个点集，能够用一条曲线去拟合，如果这个曲线是直线，则称为线性回归
- 方法：最小二乘法、梯度下降法、岭回归、LASSO

#### 1. 最小二乘法

- 基于均方误差最小化获得参数估计结果

- 适用条件

  - 数据量比较少
  - 矩阵满秩满秩

  在满足上面两个条件时，通过矩阵运算可以较快得到解析解。

#### 2. 梯度下降

​	针对最小二乘法无法得到解析解的情况下，可以利用迭代法进行求解：梯度下降（一次逼近）、牛顿（二次逼近）

- 计算代价函数当前位置的梯度计算下一次参数更新的方向

- 梯度下降分类

  - 批量梯度下降(BGS)

    因为每一次的更新需要利用所有样本的数据进行计算，当数据量**很大**时，迭代速率**很慢**，当然它的迭代过程比较稳定

  - 随机梯度下降(SGD)

    每一次的更新仅利用某一个参数更新，计算速率**快**，但迭代过程比较**曲折**

  - 小批量梯度下降(MSGD) -- 两者的结合

    选取部分样本数据进行参数更新

  - Adagrad 

    前三种方法存在无法实现学习率动态变化的缺点

    Adagrad 利用历史的梯度信息作为学习率的分母实现学习率的动态变化，但是在**训练后期**，由于分母很大会使得学习率趋于0，学习速率**很慢**

#### 3. 岭回归(Ridge Regression)

​	其实是一种改良的最小二乘，抛弃了最小二乘的无偏性，以牺牲部分信息为代价获得回归系数，通过加一个L2正则项来约束代价函数，也就是对要估计的参数进行权衡(即权衡方差) 。岭回归的罚约束项可以收缩待估系数趋于零，但并非是零。这个缺点对模型精度影响不大，但是给模型解释造成了困难（即，岭回归虽然**减少了模型的复杂度**，但并**没有解决**变量的**选择问题**）

#### 4. LASSO

​	一种压缩估计方法，通过构造罚函数得到一个较为精炼的模型，使得它能够压缩一些系数，同时设定一些系数为零，因此保留了子集收缩的优点（**特征选择**），是一种处理复共线性数据的有偏估计。

​	由于LASSO所加的罚约束项为L1范数，导函数不连续（无法进行求导并使用GD），导致在求解回归系数是**计算更为复杂**，在实际应用中，可以利用坐标下降法进行计算。

------

------

### *逻辑回归(Logistic Regression)*

​	与多重线性回归分析有很多相同之处，模型形式基本相同：$wx+b$，区别在于应变量：$y=wx+b$, 而Logistic Regression 是通过将$wx+b$对应于一个隐状态，$p=L(wx+b)$，根据$p$与$1-p$的大小决定应变量的值。如果$L()$是Logistic 函数，则称为逻辑回归。

***逻辑回归虽然叫回归，但是它其实更是一种分类方法***

- 优点：
  - 直接对分类可能性进行建模，无需事先假设数据分布，避免了假设不存在所带来的问题（**判别式模型**）
  - 不是仅预测出**“类别”**，而是得到近似的概率预测，对许多需要用概率辅助的任务很有用
  - 任意阶可导函数，有很好的数学性质




------

------

### *决策树(Decision Tree)*

​	遵循简单直观的分而治之策略，目的是生成一棵泛化能力强的决策树。简单来讲，就是通过if/else的嵌套实现。

#### 1. 核心问题

- 关键是确定**根节点**和**分支准则**
- 停止生长时机的选择

#### 2. 划分准则

##### (1) 信息熵：

度量样本集合纯度常用的一种指标

#####　(2)信息增益（ID3）

##### (3)信息增益率（C4.5）

##### (4) 基尼系数(CART) - Classification and regression tree

#### 3. 减枝策略

##### (1) 预剪枝

##### (2) 后剪枝

#### 4. 连续值与缺失值的处理

#### 5. 多变量决策树

------

------

### *贝叶斯分类器（Bayesian Classification)*

#### 1. 贝叶斯决策论

​	寻找一个判定准则$h$，使得总体风险$R(h)$最小

#### 2. 极大似然估计(MLE)

​	假定具有某种特定的概率分布形式，再基于训练样本对概率分布的参数进行估计

- 模型的训练过程就是参数的估计过程
  - 虽然参数化的估计方法使得类条件概率估计变得简单，但估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的数据分布

#### 3. 朴素贝叶斯分类器

- **最重要的前提：*样本的属性间是相互独立的***


------

------

### *支持向量机 SVM* - Support Vector Machine

#### 1. 间隔与支持向量

找到一个超平面将不同类别的样本分开

超平面线性方程为：

$w^T x + b = 0$

空间中任意样本$x$到超平面的距离为：

$r = \frac{\left | w^T x + b \right |}{\left || w \right ||}$

#### 2. 软件间隔与正则化

- 现实任务中很难确定核函数
- 即使恰好找到某个核函数，很难确定貌似线性可分割的结果不是过拟合造成的

#### 3. 核函数

#### 4.支持向量回归 

------

------

###  *集成学习*  - Ensemble learning

#### 1. 集成学习概述

#### 2. boosting 

#### 3. bagging

#### 4. 投票法、平均法 




































### 













