# Visualizing and Understanding Convolutional Networks

------

## 1. 概述

本文的主要贡献点是对卷积神经网络的隐藏层的特征进行可视化，进而通过可视化结果对当前构建的网络结构进行优化。除此之外，本文还描述了该网络中每层对整体分类性能的贡献。



## 2. 利用反卷积实现特征可视

反卷积可以看成是卷积网络的逆过程，反卷积首次提出是为了实现无监督学习，但在本文中的作用仅仅是用于可视化一个已经训练好的卷及网络模型，没有学习训练过程。

- **反卷积可视化**是以各层得到的feature map 作为输入，进行反卷积得到结果用于验证各层得到的特征结果

### a. 反池化

池化过程是不可逆的，但是可以记录池化过程中最大激活值得位置。然后在反池化的过程中，对最大激活值对应的位置激活，其他位置置零，得到一个近似的结果

![](D:\git\note\zhangbron\paper_reading\img\unpooling.png)

### b. 反激活

利用Relu函数对upooling的结果进行处理

### c. 反卷积

对于反卷积过程，采用卷积过程转置后的滤波器(参数一样，把参数矩阵水平和垂直方向翻转了一下)

![](D:\git\note\zhangbron\paper_reading\img\deconvolving.png)

------

## 3. 卷积网络可视化

### a. feature visualization

通过CNN可以学习到具有辨别性的特征。e.g. 要区分猫和狗， 通过CNN学习后，图片的背景部分的激活度会比较少，这可以通过可视化验证所提取到的特征忽略了背景。一般的，低层的Layer学习到的特征是一些基本的颜色、边缘等低层特征，后面的Layer就会学到更加复杂的、具有区别性的特征。

### b. 特征的学习过程

在网络的训练过程中，(1) 每一层学习到的特征会发生相应的变化，当出现最强的激活时，会发生sudden jumps；(2) 层与层：低层的训练过程容易收敛，而高层的特征学习变化很大，一般需要迭代很多次才能收敛。

### c. 特征不变性 - Feature Invariance

对图片进行平移、旋转和缩放操作。很小的微变会对低层的输出产生很大影响，越往高层，平移和尺度变化对最终的结果影响较小。卷积网络无法对旋转操作产生不变性，除非物体具有很强的对称性



------

### 总结

其实这篇文章的核心是反卷积网络

- 在扩充训练集的时候，调整图像角度是关键，不需要过多的将图像切割成多片进行训练（如将图像切成左上、右上、左下、右下、中五片这种形式）；
- 仔细考虑每个层对其他层的影响，可适当精简层，特别是全连接层；
- 可先进行其他数据集的预训练。



